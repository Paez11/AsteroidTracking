victor11@hostnamectl:/opt/hadoop-3.4.1/spark-3.5.4$ ./sbin/start-master.sh 
victor11@hostnamectl:/opt/hadoop-3.4.1/spark-3.5.4$ ./sbin/start-workers.sh

spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.4 --master spark://192.168.11.10:7077 /opt/kafka/AsteroidTracking/spark_asteroidTrackingProcessing.py

curl http://localhost:8083/connectors

curl -X POST -H "Content-Type: application/json" --data @/opt/kafka/AsteroidTracking/config/hdfs3-asteroids-sink-connector.json http://localhost:8083/connectors


# JMX


cp /opt/kafka_2.13-3.9.0/bin/kafka-server-start.sh /opt/kafka_2.13-3.9.0/bin/kafka-server-start_asteroidTracking_mon_kafka.sh

nano /opt/kafka_2.13-3.9.0/bin/kafka-server-start_asteroidTracking_mon_kafka.sh

cp /opt/prometheus-2.53.4/prometheus.yml /opt/prometheus-2.53.4/prometheus_asteroidTracking_mon_kafka.yml

#Generamos un cluster UUID y los IDs de los controllers
KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"
CONTROLLER_UUID="$(bin/kafka-storage.sh random-uuid)"

#Formateamos los directorios de log indicando los controllers iniciales
#sudo rm -r /opt/kafka/AsteroidTracking/logs/*
#controller
bin/kafka-storage.sh format --cluster-id ${KAFKA_CLUSTER_ID} \
                     --initial-controllers "1@localhost:9096:${CONTROLLER_UUID}" \
                     --config /opt/kafka/AsteroidTracking/config/controller.properties

#Formateamos los directorios de log de los brokers
bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c /opt/kafka/AsteroidTracking/config/broker1.properties
bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c /opt/kafka/AsteroidTracking/config/broker2.properties

#Ejecutamos los servidores Kafka (uno en cada terminal)
#Observa en el log al arrancar que indica que Levanta JMX y el puerto correspondiente al node.id
bin/kafka-server-start_asteroidTracking_mon_kafka.sh /opt/kafka/AsteroidTracking/config/controller.properties
bin/kafka-server-start_asteroidTracking_mon_kafka.sh /opt/kafka/AsteroidTracking/config/broker1.properties
bin/kafka-server-start_asteroidTracking_mon_kafka.sh /opt/kafka/AsteroidTracking/config/broker2.properties

sudo ss -tunelp | grep 1100*

# levantar workers

bin/connect-distributed.sh /opt/kafka/AsteroidTracking/config/worker1.properties
bin/connect-distributed.sh /opt/kafka/AsteroidTracking/config/worker2.properties
bin/connect-distributed.sh /opt/kafka/AsteroidTracking/config/worker3.properties

curl http://localhost:8083/connector-plugins

./prometheus --config.file=prometheus_asteroidTracking_mon_kafka.yml

systemctl start grafana-server

bin/kafka-topics.sh --create --topic neo_raw_data --bootstrap-server 192.168.11.10:9094 --replication-factor 2 --partitions 2

bin/kafka-topics.sh --describe --topic neo_raw_data --bootstrap-server 192.168.11.10:9094

spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.4 --master spark://192.168.11.10:7077 /opt/kafka/AsteroidTracking/scripts/spark_asteroidTrackingProcessing.py


curl -X POST -H "Content-Type: application/json" --data @/opt/kafka/AsteroidTracking/config/hdfs3-asteroids-sink-connector.json http://192.168.11.10:8083/connectors
curl http://localhost:8083/connectors

curl http://localhost:8083/connectors/hdfs3-asteroids-sink-connector/status

python3 /opt/kafka/AsteroidTracking/scripts/neo_data_producer.py

python3 /opt/kafka/AsteroidTracking/scripts/neo_generate_synthetic_data.py

mkfifo neo_stream.pipe

hdfs dfs -cat /bda/kafka/AsteroidTracking/topic=asteroid-events/partition=1/date=2025-05-21/part-*.json

sudo hdfs dfs -get /bda/kafka/AsteroidTracking/topic=asteroid-events /opt/kafka/AsteroidTracking/data

